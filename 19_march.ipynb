{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939a31f4-a0b1-42b1-8558-9383aee9f048",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33de45f-0d62-4ab7-a4a0-9f15c2a29f81",
   "metadata": {},
   "source": [
    " Min-Max Scaling is a data preprocessing technique used to normalize the range of features or variables in a dataset. It transforms the values of each feature to a specific range, typically between 0 and 1. This is achieved by subtracting the minimum value of the feature and dividing by the range (maximum value minus minimum value).\n",
    " \n",
    " |\n",
    "How is it Used in Data Preprocessing?\n",
    "Min-Max Scaling is commonly used in the preprocessing stage of machine learning projects to ensure that features contribute equally to the model, especially in distance-based algorithms (e.g., k-nearest neighbors, support vector machines) and gradient-based optimization algorithms (e.g., neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ba9a1f-df12-4741-bff3-0c30fa2e670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Size     Price\n",
      "0  0.000000  0.000000\n",
      "1  0.791667  0.714286\n",
      "2  0.250000  0.285714\n",
      "3  1.000000  1.000000\n",
      "4  0.125000  0.085714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Size': [600, 2500, 1200, 3000, 900],\n",
    "        'Price': [150000, 400000, 250000, 500000, 180000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=['Size', 'Price'])\n",
    "\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72bead-e457-4e22-84f3-33bc6b931cc7",
   "metadata": {},
   "source": [
    "# What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "# Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af535842-0087-4a83-a987-504527f48a10",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Normalization or Vector Normalization, is a feature scaling method that transforms each data point in the dataset to a unit vector. A unit vector has a magnitude (or Euclidean norm) of 1. This technique scales each feature vector (i.e., row in the dataset) so that the entire vector has a length of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9bbef-b0f2-4d0d-b52a-26bb439ddd2c",
   "metadata": {},
   "source": [
    "How Does it Differ from Min-Max Scaling?\n",
    "\n",
    "Range vs. Magnitude:\n",
    "Min-Max Scaling adjusts each feature to a specific range (commonly [0, 1]) based on the minimum and maximum values of that feature across all data points.\n",
    "Unit Vector Scaling adjusts each data point (i.e., row) so that its overall magnitude is 1, maintaining the direction of the data point in feature space but normalizing its length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd52410-6849-4d33-817d-302814d0d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0  0.600000  0.800000\n",
      "1  0.447214  0.894427\n",
      "2  0.640184  0.768221\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Feature1': [3, 1, 5],\n",
    "        'Feature2': [4, 2, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Unit Vector Scaling (Normalization)\n",
    "normalizer = Normalizer()\n",
    "normalized_data = normalizer.fit_transform(df)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=['Feature1', 'Feature2'])\n",
    "\n",
    "print(normalized_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593768ad-a4d7-4104-81f3-d632ce1686ae",
   "metadata": {},
   "source": [
    "# What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "# example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac42df-634c-4997-88f1-a65fcfa86e0c",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA)\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving as much variance as possible. It does this by transforming the original features into a new set of uncorrelated features called principal components, which are ordered by the amount of variance they capture from the data.\n",
    "\n",
    "How PCA is Used in Dimensionality Reduction:\n",
    "Standardize the Data: PCA requires the data to be centered around zero, so features are typically standardized before applying PCA.\n",
    "Compute the Covariance Matrix: This matrix captures the relationships between different features.\n",
    "Calculate Eigenvectors and Eigenvalues: Eigenvectors determine the direction of the principal components, and eigenvalues determine their magnitude (importance).\n",
    "Select Principal Components: The top components (with the highest eigenvalues) are selected to reduce dimensionality, keeping the most important information.\n",
    "Transform the Data: The original data is projected onto the selected principal components to obtain a lower-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb420dd-5579-4a16-b715-840e6b2bcc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2\n",
      "0 -1.180178 -0.288121\n",
      "1  2.961279  0.123194\n",
      "2 -1.628910  0.512043\n",
      "3 -0.469285  0.181215\n",
      "4 -2.604368 -0.298370\n",
      "5 -1.455327  0.223483\n",
      "6  0.098673 -0.418672\n",
      "7  1.545265  0.188687\n",
      "8  0.695080  0.024176\n",
      "9  2.037771 -0.247635\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Feature1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],\n",
    "        'Feature2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9],\n",
    "        'Feature3': [1.5, 0.2, 1.8, 1.4, 2.0, 1.7, 1.2, 0.9, 1.0, 0.5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA to reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "pca_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(pca_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04776da4-600d-44f5-9b4e-49752a5b2e13",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "# Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088c42e-0f31-4da4-b706-753d2c7fe0e9",
   "metadata": {},
   "source": [
    "Relationship Between PCA and Feature Extraction\n",
    "Principal Component Analysis (PCA) and Feature Extraction are closely related concepts.\n",
    "\n",
    "Feature Extraction involves creating new features from the existing ones to capture the most important information. It aims to reduce the dimensionality of the data while retaining essential characteristics.\n",
    "\n",
    "PCA is a specific method used for feature extraction. It transforms the original features into a new set of uncorrelated features (principal components) that capture the maximum variance in the data. These principal components are linear combinations of the original features and are ordered by the amount of variance they explain.\n",
    "\n",
    "How PCA Can Be Used for Feature Extraction\n",
    "PCA can be used for feature extraction by selecting the top principal components that capture the most variance. These components can then be used as new features in machine learning models, potentially improving performance by reducing noise and multicollinearity in the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be49a82-df69-4342-a478-b69ca4672562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1           PC2\n",
      "0  2.828427  3.648565e-16\n",
      "1  1.414214 -1.216188e-16\n",
      "2 -0.000000  0.000000e+00\n",
      "3 -1.414214  1.216188e-16\n",
      "4 -2.828427  2.432377e-16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [2, 4, 6, 8, 10],\n",
    "        'Feature3': [5, 4, 3, 2, 1],\n",
    "        'Feature4': [10, 9, 8, 7, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA to extract 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "pca_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(pca_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea1b85-2db0-4e21-ac6a-70c9c2237db8",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "# contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "# preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10bfab1-bcdb-4433-8bdb-43d59504ef0e",
   "metadata": {},
   "source": [
    "Using Min-Max Scaling in this context ensures that features with different ranges are normalized, which prevents any one feature (like price with potentially large numbers) from disproportionately influencing the recommendation system's performance. This preprocessing step is crucial for building effective models that make balanced recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a00df2-30aa-4e8b-ac02-e94464b351ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price    rating  delivery_time\n",
      "0   0.00  0.583333       0.333333\n",
      "1   0.50  0.000000       0.833333\n",
      "2   0.25  0.166667       0.000000\n",
      "3   1.00  1.000000       1.000000\n",
      "4   0.75  0.333333       0.666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'price': [10, 20, 15, 30, 25],\n",
    "        'rating': [4.5, 3.8, 4.0, 5.0, 4.2],\n",
    "        'delivery_time': [30, 45, 20, 50, 40]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max Scaling to the dataset\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert scaled data to a DataFrame for readability\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=['price', 'rating', 'delivery_time'])\n",
    "\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10849039-5fe3-41bf-a0db-2a8056c43dc9",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "# features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "# dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e76695-fffa-4914-9b9e-e5bf800bbf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2\n",
      "0  3.890336 -0.261988\n",
      "1 -0.390455 -0.479979\n",
      "2  2.037196  0.170841\n",
      "3 -3.793704 -0.721570\n",
      "4 -1.743372  1.292696\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset with financial and market features\n",
    "data = {\n",
    "    'Revenue': [100, 200, 150, 300, 250],\n",
    "    'Profit': [10, 20, 15, 30, 25],\n",
    "    'Assets': [500, 600, 550, 700, 650],\n",
    "    'Liabilities': [200, 220, 210, 240, 230],\n",
    "    'Market_Cap': [1000, 1100, 1050, 1150, 1200],\n",
    "    'Volume': [10000, 15000, 12000, 16000, 14000],\n",
    "    'PE_Ratio': [15, 16, 15.5, 17, 16.5],\n",
    "    'Dividend_Yield': [3, 2.5, 3, 2, 2.8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "pca_df = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])\n",
    "\n",
    "print(pca_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a2250-add0-4ad1-9d7e-6c723213d450",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d7d9b9-0f68-4f64-aa63-41c7359f4a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min-Max Scaling parameters\n",
    "min_range = -1\n",
    "max_range = 1\n",
    "\n",
    "# Min and Max of the original data\n",
    "data_min = data.min()\n",
    "data_max = data.max()\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaled_data = (data - data_min) * (max_range - min_range) / (data_max - data_min) + min_range\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c911d8-5dd0-4740-8c78-300c3e0bcb23",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672dce8b-bea2-4579-b5e5-0a45a5c80b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain: 3\n",
      "Explained variance by each component: [5.34257167e-01 2.96725017e-01 1.56550356e-01 1.24674603e-02\n",
      " 9.00734575e-36]\n",
      "Cumulative explained variance: [0.53425717 0.83098218 0.98753254 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Sample DataFrame with hypothetical data\n",
    "data = {'height': [170, 160, 180, 175, 165],\n",
    "        'weight': [70, 60, 80, 75, 65],\n",
    "        'age': [25, 30, 35, 40, 45],\n",
    "        'gender': ['male', 'female', 'male', 'female', 'male'],\n",
    "        'blood_pressure': [120, 130, 110, 140, 125]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing: One-Hot Encoding for 'gender'\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['height', 'weight', 'age', 'blood_pressure']),\n",
    "        ('cat', OneHotEncoder(drop='first'), ['gender'])\n",
    "    ])\n",
    "\n",
    "scaled_data = preprocessor.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "\n",
    "# Determine how many components to keep for 95% variance\n",
    "n_components = next(i for i, total in enumerate(cumulative_variance) if total >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of components to retain: {n_components}\")\n",
    "print(f\"Explained variance by each component: {explained_variance}\")\n",
    "print(f\"Cumulative explained variance: {cumulative_variance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
