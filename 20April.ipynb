{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14aa59f5-84b8-43ba-a811-4f693d637d6a",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks.\n",
    "\n",
    " It operates on the principle that similar data points tend to be close to each other in the feature space.   \n",
    "\n",
    "To classify a new data point, KNN finds the K closest data points (neighbors) from the training set based on a distance metric (like Euclidean distance) and assigns the class label that is most frequent among these neighbors. For regression, the average of the target values of the K nearest neighbors is predicted as the output.   \n",
    "\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing the optimal value of K is crucial for KNN's performance. A small K can be sensitive to noise, while a large K might smooth out decision boundaries too much.   \n",
    "\n",
    "Common techniques for selecting K include:\n",
    "\n",
    "Cross-validation: Experiment with different K values and choose the one that yields the best performance on a validation set.\n",
    "Error rate curve: Plot error rate against different K values and select the value where the error rate stabilizes.   \n",
    "Domain knowledge: Incorporate insights from the problem domain to guide the choice of K.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "KNN Classifier: Predicts the class label of a new data point based on the majority class of its K nearest neighbors.   \n",
    "KNN Regressor: Predicts the numerical value of a new data point based on the average of the target values of its K nearest neighbors.   \n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "The choice of performance metric depends on the problem type:\n",
    "\n",
    "Classification: Accuracy, precision, recall, F1-score, confusion matrix.\n",
    "Regression: Mean squared error (MSE), mean absolute error (MAE), R-squared.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates as the number of features increases. This is because the distance calculations become less meaningful in high-dimensional spaces.   \n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Several approaches can be used:\n",
    "\n",
    "Imputation: Fill missing values with mean, median, or mode of the corresponding feature.\n",
    "Dropping: Remove instances with missing values.\n",
    "KNN imputation: Use KNN itself to predict missing values based on similar instances.   \n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "KNN Classifier: Generally performs well with small to medium-sized datasets and clear decision boundaries. It's suitable for classification problems.   \n",
    "KNN Regressor: Can be effective for regression tasks, but its performance can be impacted by noise and outliers. It might not be as accurate as other regression algorithms for complex relationships.\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Simple to understand and implement.   \n",
    "No training phase, making it efficient for new data points.   \n",
    "Effective for non-linear relationships.   \n",
    "Weaknesses:\n",
    "\n",
    "Can be computationally expensive for large datasets.   \n",
    "Sensitive to the choice of K and distance metric.\n",
    "Prone to the curse of dimensionality.\n",
    "Addressing weaknesses:\n",
    "\n",
    "Use efficient data structures (e.g., KD-trees) for faster neighbor search.\n",
    "Conduct hyperparameter tuning to find optimal K and distance metric.\n",
    "Consider dimensionality reduction techniques (e.g., PCA) to mitigate the curse of dimensionality.\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "\n",
    "Euclidean distance: Straight-line distance between two points in Euclidean space.   \n",
    "Manhattan distance: Sum of absolute differences between corresponding coordinates.\n",
    "The choice of distance metric depends on the problem and the nature of the features. Euclidean distance is commonly used for continuous features, while Manhattan distance might be suitable for categorical or ordinal features.\n",
    "\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "\n",
    "Feature scaling is essential in KNN to prevent features with larger scales from dominating the distance calculations. By scaling features to a common range, you ensure that all features contribute equally to the distance metric. Common scaling techniques include normalization and standardization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
