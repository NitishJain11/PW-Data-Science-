{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05fcc60-e6a4-4e50-9a44-7763d4c9c8db",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "\n",
    "\n",
    "Euclidean vs. Manhattan Distance in KNN\n",
    "Main Difference\n",
    "Euclidean distance calculates the straight-line distance between two points in Euclidean space.\n",
    "\n",
    " It's essentially the Pythagorean theorem in higher dimensions.   \n",
    "Manhattan distance calculates the distance between two points by summing the absolute differences of their Cartesian coordinates. It's often referred to as the \"city block distance\" as it represents the distance a car would travel in a city with a rectangular grid.   \n",
    "Impact on KNN Performance\n",
    "The choice of distance metric can significantly impact the performance of a KNN classifier or regressor.   \n",
    "\n",
    "Euclidean distance is generally more sensitive to outliers as it squares the differences between coordinates. This can be beneficial when outliers are informative and should influence the model. However, if the data contains many outliers, it might lead to suboptimal results.\n",
    "Manhattan distance is less sensitive to outliers as it uses absolute differences. This can be advantageous when dealing with datasets containing noise or outliers. It might also be more suitable for data where the directionality of the axes is important, such as time-series data or data with ordinal features.   \n",
    "In summary, the choice between Euclidean and Manhattan distance depends on the specific characteristics of the dataset and the problem at hand. Experimentation with both metrics is often necessary to determine the best choice for a given scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0cea05-91de-4318-b14a-9bbcf0dda67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean Distance (Wine Dataset): 0.74\n",
      "Accuracy with Manhattan Distance (Wine Dataset): 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Implement KNN with Euclidean Distance\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "print(f\"Accuracy with Euclidean Distance (Wine Dataset): {accuracy_euclidean:.2f}\")\n",
    "\n",
    "# Implement KNN with Manhattan Distance\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "print(f\"Accuracy with Manhattan Distance (Wine Dataset): {accuracy_manhattan:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a0c7f-24f3-443b-af68-d0abacfb5531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cc3515b-3196-4b7a-a0c8-72683045ae5e",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "The optimal value of k in KNN is crucial for model performance. A small k can be sensitive to noise, while a large k might smooth out decision boundaries too much.\n",
    "\n",
    "Techniques to determine optimal k:\n",
    "\n",
    "Cross-validation: Split the data into training and validation sets. Experiment with different k values and select the one that yields the best performance on the validation set.\n",
    "Error rate curve: Plot the error rate against different k values and choose the value where the error rate stabilizes.\n",
    "Domain knowledge: Incorporate insights from the problem domain to guide the choice of k.\n",
    "Rule of thumb: A common starting point is to set k equal to the square root of the number of data points.\n",
    "\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might \n",
    "you choose one distance metric over the other?\n",
    "\n",
    "The choice of distance metric significantly impacts KNN performance.   \n",
    "\n",
    "Euclidean distance is sensitive to outliers and works well for continuous features.\n",
    "Manhattan distance is less sensitive to outliers and can be suitable for data with ordinal or categorical features.\n",
    "Consider using:\n",
    "\n",
    "Euclidean distance when features are continuous and normally distributed.\n",
    "Manhattan distance when features are ordinal or categorical, or when the data contains outliers.\n",
    "Other distance metrics (e.g., Minkowski, Chebyshev) for specific use cases.\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "Hyperparameters in KNN:\n",
    "\n",
    "k: Number of neighbors (already discussed).\n",
    "Distance metric: Choice of distance calculation (Euclidean, Manhattan, etc.).\n",
    "Weighting: Assigns weights to neighbors (uniform or distance-based).\n",
    "Tuning hyperparameters:\n",
    "\n",
    "Grid search: Experiment with different combinations of hyperparameters and evaluate performance.\n",
    "Random search: Randomly sample hyperparameter values and evaluate performance.\n",
    "Cross-validation: Use cross-validation to assess model performance on different data splits.\n",
    "\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "\n",
    "Larger training sets generally improve KNN performance by providing more information to the model. However, increasing the training set size also increases computational cost.\n",
    "\n",
    "Techniques to optimize training set size:\n",
    "\n",
    "Feature selection: Remove irrelevant features to reduce dimensionality.\n",
    "Data cleaning: Handle missing values and outliers to improve data quality.\n",
    "Oversampling/undersampling: Balance class distribution in imbalanced datasets.\n",
    "Dimensionality reduction: Reduce the number of features using techniques like PCA.\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "Computational cost: Can be slow for large datasets.\n",
    "Sensitive to noise and outliers: Can be affected by noisy data.\n",
    "Curse of dimensionality: Performance degrades in high-dimensional spaces.\n",
    "Overcoming drawbacks:\n",
    "\n",
    "Efficient data structures: Use KD-trees or ball trees for faster neighbor search.\n",
    "Outlier detection and removal: Identify and remove outliers to improve data quality.\n",
    "Dimensionality reduction: Apply techniques like PCA to reduce feature space.\n",
    "Ensemble methods: Combine KNN with other algorithms for improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad59c7b-a045-44f3-96eb-341892facc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
