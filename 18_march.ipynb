{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f7ab43-ff7d-47df-bf34-2beda6ac7a59",
   "metadata": {},
   "source": [
    "What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1529b-97b3-4fba-8590-7d39393d4fa0",
   "metadata": {},
   "source": [
    "# Filter Methods in Feature Selection\n",
    "# Filter methods are a technique in machine learning used to select relevant features by evaluating their relationship with the target variable without involving any learning algorithm. They are computationally efficient and serve as a preliminary step before applying more complex methods.   \n",
    "\n",
    "# How Filter Methods Work\n",
    "# Assign a score to each feature:\n",
    "# Based on its correlation with the target variable.   \n",
    "# Considering the feature's intrinsic properties like variance or distribution.   \n",
    "# Rank features:\n",
    "# Order the features based on their assigned scores.   \n",
    "# Select features:\n",
    "# Choose the top-ranked features as the final feature set.\n",
    "# Popular Filter Methods\n",
    "# Correlation Coefficient: Measures the linear relationship between two variables.   \n",
    "# Chi-Square Test: Evaluates the independence between categorical features and the target variable.   \n",
    "# Information Gain: Measures the decrease in entropy (uncertainty) after splitting the data based on a feature.   \n",
    "# Fisher's Score: Evaluates the separability of classes based on a feature.\n",
    "# Variance Threshold: Removes features with low variance.   \n",
    "# Advantages of Filter Methods\n",
    "# Computationally efficient.   \n",
    "# Independent of the learning algorithm.   \n",
    "# Can be used as a preprocessing step for other feature selection techniques.   \n",
    "# Limitations of Filter Methods\n",
    "# Might not capture complex interactions between features.   \n",
    "# Can be less accurate than wrapper or embedded methods in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2f09c-2c93-4fc8-9ecf-68938655500d",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3257f-5e95-45fb-b297-46491b6ad0d5",
   "metadata": {},
   "source": [
    "# Filter vs. Wrapper Methods in Feature Selection\n",
    "# Filter methods and wrapper methods are two primary approaches to feature selection in machine learning. They differ significantly in their methodologies and computational costs.   \n",
    "\n",
    "# Filter Methods\n",
    "# Independent of the model: Evaluate features based on intrinsic properties or statistical measures.   \n",
    "# Faster: Computationally efficient as they don't involve training models.   \n",
    "# Less accurate: Often less precise in identifying the optimal feature subset compared to wrapper methods.   \n",
    "# Examples: Correlation coefficient, Chi-square test, Information Gain, Fisher's Score.   \n",
    "# Wrapper Methods\n",
    "# Dependent on the model: Use a specific machine learning algorithm to evaluate the performance of different feature subsets.   \n",
    "# Slower: Computationally expensive as they involve training multiple models.   \n",
    "# More accurate: Often achieve better performance by considering the interaction between features.\n",
    "# Examples: Forward selection, backward elimination, recursive feature elimination.   \n",
    "# In essence:\n",
    "\n",
    "# Filter methods are like a pre-screening process, quickly narrowing down features based on general criteria.   \n",
    "# Wrapper methods are more exhaustive, trying different combinations of features and selecting the best subset based on model performance.   \n",
    "# The choice between filter and wrapper methods depends on factors such as:\n",
    "\n",
    "# Dataset size\n",
    "# Computational resources\n",
    "# Desired level of accuracy\n",
    "# Complexity of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a626c4-90e6-4ea3-ab0b-d0fc5c4f84c5",
   "metadata": {},
   "source": [
    "What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fceee-6fbf-4252-a4af-76e24e8b9266",
   "metadata": {},
   "source": [
    "# Embedded Feature Selection Methods\n",
    "# Embedded methods combine the strengths of filter and wrapper methods by performing feature selection as part of the model training process. They often yield better results as they consider the interaction between features and the target variable.   \n",
    "\n",
    "# Common Techniques\n",
    "# Regularization:\n",
    "\n",
    "# Lasso (L1 regularization): Shrinks coefficients of less important features to zero, effectively removing them.   \n",
    "# Ridge (L2 regularization): Reduces the impact of correlated features but doesn't eliminate them.   \n",
    "# Elastic Net: Combines L1 and L2 for a balance between feature selection and shrinkage.   \n",
    "# Tree-based Methods:\n",
    "\n",
    "# Random Forest: Calculates feature importance based on the number of times a feature is used to split nodes.\n",
    "# Gradient Boosting: Assigns importance scores to features based on their contribution to the model's performance.\n",
    "# Recursive Feature Elimination (RFE):\n",
    "\n",
    "# While not strictly an embedded method, it's often used in conjunction with other embedded techniques.\n",
    "# Recursively removes features based on their importance scores assigned by a model.   \n",
    "# Advantages of Embedded Methods\n",
    "# Consider the interaction between features and the target variable.\n",
    "# Often provide better performance than filter methods.\n",
    "# Efficient compared to wrapper methods.\n",
    "# Disadvantages\n",
    "# Can be computationally expensive for complex models.   \n",
    "# Might be biased towards the chosen model.\n",
    "# In summary, embedded methods offer a balance between computational efficiency and accuracy in feature selection. They are particularly useful when dealing with complex datasets and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3b568-5cd0-410f-b8fe-c4c06e6100ce",
   "metadata": {},
   "source": [
    "What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509acdb-7110-4676-b450-bcbfe160d472",
   "metadata": {},
   "source": [
    "# Drawbacks of Filter Methods for Feature Selection\n",
    "# While filter methods offer a quick and efficient way to reduce dimensionality, they have some limitations:\n",
    "\n",
    "# Ignore feature interactions: Filter methods typically evaluate features independently, neglecting potential interactions between them. This can lead to suboptimal feature subsets.   \n",
    "# Limited to univariate relationships: They primarily focus on the relationship between a single feature and the target variable, overlooking multivariate dependencies.\n",
    "# Sensitive to feature scaling: The performance of some filter methods can be affected by the scale of features, requiring careful preprocessing.\n",
    "# Might not capture complex patterns: For datasets with intricate relationships between features, filter methods might not be sufficient to identify the most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc1d4a-cb71-43ca-926d-930b6e420d02",
   "metadata": {},
   "source": [
    "In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72446b-cf1b-4cc1-bb1d-78285081d576",
   "metadata": {},
   "source": [
    "# Filter methods, while less accurate than wrapper methods, have their strengths in specific situations:\n",
    "\n",
    "# Large Datasets\n",
    "# When dealing with massive datasets, the computational cost of wrapper methods can be prohibitive. Filter methods are significantly faster and can be used as a preliminary step to reduce dimensionality before applying more complex techniques.   \n",
    "# High-Dimensional Data\n",
    "# In cases where the number of features is extremely large, filter methods can be more efficient in identifying a subset of relevant features. Wrapper methods might be computationally infeasible.\n",
    "# Limited Computational Resources\n",
    "# If computational power is constrained, filter methods are a practical choice as they require fewer resources.\n",
    "# Understanding Feature Importance\n",
    "# Filter methods can provide insights into the intrinsic importance of features, which can be helpful in understanding the underlying data.   \n",
    "# Initial Feature Screening\n",
    "# Filter methods can be used as a preprocessing step to quickly eliminate irrelevant features before applying more sophisticated techniques like wrapper or embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4a358-a077-4748-add1-b0495aa355df",
   "metadata": {},
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8245aa5-1ab1-45de-9b3e-e05ecc75dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This psudo code provides a basic approach to feature selection using the Filter Method, which you can adapt and extend based on \n",
    "#specific dataset and requirements.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import chi2, SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('churn_data.csv')\n",
    "\n",
    "# Assuming 'Churn' is the target variable\n",
    "target = 'Churn'\n",
    "X = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "# Preprocessing: Encode categorical variables and scale numerical features\n",
    "label_encoders = {}\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[column] = le.fit_transform(X[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Apply Filter Method: Using Chi-Square for categorical variables\n",
    "# and Mutual Information for both numerical and categorical variables\n",
    "# Selecting top k features\n",
    "k = 10  # Number of features to select\n",
    "\n",
    "# Chi-Square for categorical data\n",
    "chi2_selector = SelectKBest(chi2, k=k)\n",
    "X_chi2_selected = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "# Mutual Information for mixed data\n",
    "mi_selector = SelectKBest(mutual_info_classif, k=k)\n",
    "X_mi_selected = mi_selector.fit_transform(X, y)\n",
    "\n",
    "# Combining selected features from both methods (example, you can choose either or both)\n",
    "X_selected = pd.DataFrame(X_mi_selected, columns=X.columns[mi_selector.get_support()])\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a model using the selected features\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Selected Features: {X_selected.columns.tolist()}\")\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392d03d-d32c-4854-b0b4-6f0506294f6b",
   "metadata": {},
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b1285-fef7-4a4f-a878-fc3465fa3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Embedded Method integrates feature selection into the model training process, making it an efficient approach to identifying the most relevant features.\n",
    "# By using models like Lasso Regression and Random Forest, you can automatically select features that are most predictive of the soccer match outcome, leading \n",
    "# to better-performing models.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('soccer_match_data.csv')\n",
    "\n",
    "# Assuming 'Outcome' is the target variable (e.g., 0 for loss, 1 for win)\n",
    "target = 'Outcome'\n",
    "X = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "# Preprocessing: Encode categorical variables and scale numerical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Embedded Method 1: Lasso Regression for feature selection\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "lasso_selected_features = X_train.columns[lasso.coef_ != 0]\n",
    "X_train_lasso_selected = X_train[lasso_selected_features]\n",
    "X_test_lasso_selected = X_test[lasso_selected_features]\n",
    "\n",
    "# Evaluate Lasso-selected features using a model (e.g., Random Forest)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_lasso_selected, y_train)\n",
    "rf_accuracy = cross_val_score(rf_model, X_test_lasso_selected, y_test, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Features selected by Lasso Regression: {lasso_selected_features.tolist()}\")\n",
    "print(f\"Random Forest model accuracy with Lasso-selected features: {np.mean(rf_accuracy):.2f}\")\n",
    "\n",
    "# Embedded Method 2: Feature importance from Random Forest\n",
    "rf_model.fit(X_train, y_train)\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Select top features based on importance\n",
    "top_features = X_train.columns[indices][:10]  # Select top 10 features as an example\n",
    "X_train_rf_selected = X_train[top_features]\n",
    "X_test_rf_selected = X_test[top_features]\n",
    "\n",
    "# Evaluate with selected features\n",
    "rf_accuracy_top_features = cross_val_score(rf_model, X_test_rf_selected, y_test, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Top features selected by Random Forest: {top_features.tolist()}\")\n",
    "print(f\"Random Forest model accuracy with top selected features: {np.mean(rf_accuracy_top_features):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09342e-fad5-4fc7-88fe-237a7c3fe8bb",
   "metadata": {},
   "source": [
    "You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c1870-35a8-443f-8cd6-5c88d524572b",
   "metadata": {},
   "source": [
    "# The Wrapper Method is effective for feature selection because it evaluates the impact of each feature subset on model performance. However, it can be computationally expensive, especially with large datasets or complex models. Recursive Feature Elimination (RFE) is a practical approach within this method, allowing for an iterative selection process that directly optimizes model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be8d8c-bc26-4900-8bca-840f1b86ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('house_prices.csv')\n",
    "\n",
    "# Assuming 'Price' is the target variable\n",
    "target = 'Price'\n",
    "X = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "# Preprocessing: Encode categorical variables and scale numerical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Base Model: Linear Regression\n",
    "base_model = LinearRegression()\n",
    "\n",
    "# Wrapper Method: Recursive Feature Elimination (RFE)\n",
    "n_features_to_select = 5  # Number of features to select\n",
    "rfe = RFE(base_model, n_features_to_select=n_features_to_select)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "\n",
    "# Evaluate the model with selected features\n",
    "X_train_rfe_selected = X_train[selected_features]\n",
    "X_test_rfe_selected = X_test[selected_features]\n",
    "\n",
    "# Fit the model on the selected features\n",
    "base_model.fit(X_train_rfe_selected, y_train)\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(base_model, X_test_rfe_selected, y_test, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the root mean squared error (RMSE)\n",
    "rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "print(f\"Selected Features: {selected_features.tolist()}\")\n",
    "print(f\"Model RMSE with selected features: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf05ed6-afc9-40e4-955c-fc7fd3a43883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75f6ce-50cb-4157-874c-9eba5a904fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f2c90-57ed-45aa-9dc6-310356709595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680dd08-e3a5-4274-9813-7b3a0f778047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023bb1c-e580-49ab-b172-01f7c8c9d9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
