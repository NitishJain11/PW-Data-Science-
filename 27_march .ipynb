{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526db9ef-400a-45af-8351-04524faaf763",
   "metadata": {},
   "source": [
    "# Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "# represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f142b-9452-41a9-8f88-94b7cb0e8ee7",
   "metadata": {},
   "source": [
    "\n",
    "R-squared in Linear Regression\n",
    "Concept:\n",
    "\n",
    "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It provides an indication of the goodness of fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7be401d-6b78-49bc-b282-bc521cbbd1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3929192951925171"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1, 2, 1.3, 3.75, 2.25])\n",
    "\n",
    "# Create a linear regression model and fit it\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "r_squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8a3e0-e555-4c16-be89-710641fb1085",
   "metadata": {},
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590bf9fc-b194-4d6f-b5d5-3c564b8cc735",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of the R-squared that accounts for the number of predictors (independent variables) in the model. Unlike regular R-squared, which always increases when additional predictors are added to the model (even if they don't contribute much to the model's explanatory power), adjusted R-squared adjusts for the number of predictors and only increases if the new predictor improves the model more than would be expected by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fc3bc-ab08-4ee8-bcd1-0aa6af68d9a2",
   "metadata": {},
   "source": [
    "Differences Between R-squared and Adjusted R-squared:\n",
    "R-squared: Measures the proportion of variance explained by the model but doesn't account for the number of predictors. It can increase even with irrelevant predictors.\n",
    "Adjusted R-squared: Penalizes the model for adding predictors that do not improve the model significantly. It provides a more accurate measure of the goodness of fit, especially when multiple predictors are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a9b8a9-7e39-407a-be28-2adcc4d6ba42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3929192951925169, -0.2141614096149662)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([1, 2, 1.3, 3.75, 2.25])\n",
    "\n",
    "# Create a linear regression model and fit it\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X.shape[0]  # Number of observations\n",
    "k = X.shape[1]  # Number of predictors\n",
    "adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "\n",
    "r_squared, adjusted_r_squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94064a87-2a87-4bb7-9abc-6c380c78b79b",
   "metadata": {},
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526a6fe-1719-4884-9ecd-ad7e7b9481ba",
   "metadata": {},
   "source": [
    "Use adjusted R-squared when you have multiple predictors in your model, when comparing models with different numbers of predictors, or when you want to prevent overfitting and ensure that added complexity genuinely improves your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2cfbd-fe9d-407d-b9e9-baa1b3692816",
   "metadata": {},
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345257a6-bd68-4648-bf7a-ef1d3244a132",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common metrics used to evaluate the performance of regression models. Each metric measures the difference between predicted values and actual values, but they do so in slightly different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb401-002d-47d1-9d76-87c712486aa4",
   "metadata": {},
   "source": [
    "1. Mean Squared Error (MSE)\n",
    "Explanation:\n",
    "MSE calculates the average of the squared differences between the actual and predicted values. By squaring the errors, MSE penalizes larger errors more heavily than smaller ones.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Lower MSE values indicate better model performance.\n",
    "Since MSE involves squaring the errors, it is sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d414be-4306-4b9c-af6c-82a99991a093",
   "metadata": {},
   "source": [
    "2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "Explanation:\n",
    "\n",
    "RMSE is the square root of MSE, and it gives an error metric in the same units as the dependent variable \n",
    "ùë¶\n",
    "y.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "RMSE is useful for interpreting the magnitude of the error.\n",
    "Like MSE, RMSE is sensitive to outliers but is often more intuitive because it is in the same units as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f3e12-7c0f-4819-a7c0-be1beecf7d53",
   "metadata": {},
   "source": [
    "3. Mean Absolute Error (MAE)\n",
    "\n",
    "Explanation:\n",
    "\n",
    "MAE calculates the average of the absolute differences between the actual and predicted values. Unlike MSE and RMSE, it does not square the errors, so it is less sensitive to outliers.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "MAE provides a straightforward measure of error.\n",
    "It is easier to interpret but may not penalize large errors as heavily as MSE or RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01120c9d-42a8-4e2f-8ef9-a89c77863460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.375, 0.6123724356957945, 0.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Sample data\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "mse, rmse, mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8be1e4-9d9c-4f05-93c2-730732d8bcda",
   "metadata": {},
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38c355-3a99-469a-a57e-ef237d5b8164",
   "metadata": {},
   "source": [
    ". Mean Squared Error (MSE):\n",
    "\n",
    "Advantages:\n",
    "    \n",
    "Penalizes larger errors more heavily, making it useful when large deviations are particularly undesirable.\n",
    "Commonly used and easy to compute.\n",
    "\n",
    "Disadvantages:\n",
    "    \n",
    "Sensitive to outliers due to the squaring of errors.\n",
    "Harder to interpret because it's not in the same units as the dependent variable.\n",
    "2. Root Mean Squared Error (RMSE):\n",
    "\n",
    "Advantages:\n",
    "    \n",
    "Same units as the dependent variable, making it easier to interpret.\n",
    "Penalizes large errors similarly to MSE.\n",
    "\n",
    "Disadvantages:\n",
    "    \n",
    "Still sensitive to outliers.\n",
    "Like MSE, it can be dominated by large errors.\n",
    "3. Mean Absolute Error (MAE):\n",
    "\n",
    "Advantages:\n",
    "    \n",
    "Less sensitive to outliers as it doesn't square the errors.\n",
    "Easier to interpret as it gives the average magnitude of errors in the same units as the dependent variable.\n",
    "\n",
    "Disadvantages:\n",
    "    \n",
    "Does not penalize larger errors as heavily as MSE or RMSE, which might be less ideal in cases where large errors are particularly problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59239f5b-de9c-4d86-9031-e186a74efa1c",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a5152-e59c-4872-92d2-a5d0f16f5fbb",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique used in linear regression that adds a penalty equal to the absolute value of the coefficients' magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59b196-5c51-4790-bd40-b296535c2501",
   "metadata": {},
   "source": [
    "Key Feature: Lasso can shrink some coefficients to exactly zero, effectively performing variable selection and producing a more interpretable model.\n",
    "Difference from Ridge Regularization:\n",
    "Ridge Regularization: Adds a penalty equal to the square of the coefficients' magnitudes. It shrinks coefficients but doesn't set them to zero, so all variables remain in the model.\n",
    "\n",
    "Lasso Regularization: Uses an absolute value penalty, which can set some coefficients to zero, leading to sparser models.\n",
    "\n",
    "When to Use:\n",
    "Lasso is more appropriate when: You expect that many of the features are irrelevant or when you want feature selection within the model. It's useful for creating simpler, more interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838665a8-fac0-41c2-94fd-2911c43b647d",
   "metadata": {},
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73948df-117f-4cfc-a72a-94efe2511950",
   "metadata": {},
   "source": [
    "Regularized linear models, such as Lasso and Ridge, add a penalty to the model's cost function based on the size of the coefficients. This penalty discourages the model from fitting too closely to the training data by reducing the influence of less important features. By doing so, these models help prevent overfitting, which occurs when a model is too complex and captures noise in the training data rather than the underlying trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f06902-d526-43ba-9158-d7cfc8ca6aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2951282462401497, 0.29150988312450077, 0.2680449338809307)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 10)  # 100 samples, 10 features\n",
    "y = X[:, 0] + 0.5 * np.random.randn(100)  # Only the first feature is predictive\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Regular Linear Regression (No regularization)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse_no_reg = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Ridge Regularization\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "# Lasso Regularization\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "mse_no_reg, mse_ridge, mse_lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfa8c6-adc9-4a3d-a945-9b8cb60eb522",
   "metadata": {},
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574baca1-3296-4084-9cc8-0d2342456985",
   "metadata": {},
   "source": [
    "Regularized linear models are powerful tools for preventing overfitting and handling multicollinearity, but their limitations‚Äîsuch as the assumption of linearity, sensitivity to feature scaling, and the need for careful hyperparameter tuning‚Äîmean they may not always be the best choice for all types of regression problems. Alternative models like tree-based methods or non-linear techniques might be more suitable in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e7262-e44f-4753-91b2-779b99b62d22",
   "metadata": {},
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b9a47-e9ac-46cd-97a9-99cf2e087d4c",
   "metadata": {},
   "source": [
    "Given the provided metrics, you might lean towards Model B with the lower MAE, as it suggests a lower average error. However, without comparable metrics for both models (both RMSE and MAE for each), this choice is limited and may not fully reflect the models' performance, especially concerning outliers or error distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630e95d-78f4-4c5a-a138-eb68b6d990dd",
   "metadata": {},
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f856ad5-3bb5-4c79-bd32-a0b7ad2dfaf0",
   "metadata": {},
   "source": [
    "To choose the better performer between Model A (Ridge) and Model B (Lasso):\n",
    "\n",
    "Model A (Ridge) is better if you want to retain all features and manage multicollinearity while reducing overfitting without making the model sparse.\n",
    "Model B (Lasso) is preferable if you aim for a simpler model with feature selection, where some coefficients can be set to zero, leading to better interpretability and potentially reducing the number of features.\n",
    "Trade-offs:\n",
    "Ridge: Keeps all features, which may include less relevant ones, and doesn‚Äôt simplify the model.\n",
    "Lasso: Can remove features entirely, which can simplify the model but may discard important variables, especially if features are correlated.\n",
    "Ultimately, the choice depends on whether you prioritize feature retention and multicollinearity handling (Ridge) or model simplicity and feature selection (Lasso). Evaluating both models using a consistent performance metric (e.g., RMSE, MAE) on your validation set will help determine which performs better in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27be47e-48cb-48d5-ab19-379daceb83bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
