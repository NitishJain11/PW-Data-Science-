{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff79290-286f-4883-b841-93d96d376380",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd404b-182d-41c6-98a2-cc4e2619e13b",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique that builds a predictive model in an ensemble manner, combining multiple weak predictive models to create a strong predictive model.\n",
    "\n",
    "Unlike traditional regression models, gradient boosting focuses on minimizing the errors of previous models by fitting new models to the residuals.   \n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "Ensemble method: Combines multiple weak learners (typically decision trees) to create a strong model.   \n",
    "Gradient descent: Uses a gradient descent-like approach to minimize the loss function (e.g., mean squared error) at each iteration.   \n",
    "Residual-based learning: Each new model focuses on predicting the residuals (errors) of the previous models, improving overall accuracy.   \n",
    "Iterative process: Sequentially builds models, with each new model correcting the mistakes of the previous ones.   \n",
    "\n",
    "\n",
    "How it works:\n",
    "\n",
    "An initial model (often a simple model like a constant) is created.   \n",
    "The residuals (differences between actual and predicted values) of the initial model are calculated.   \n",
    "A new model is built to predict these residuals.   \n",
    "The predictions of the new model are added to the predictions of the previous model.   \n",
    "Steps 2-4 are repeated for a specified number of iterations or until a stopping criterion is met.   \n",
    "Advantages:\n",
    "\n",
    "Often achieves high accuracy.   \n",
    "Can handle various data types.\n",
    "Less prone to overfitting compared to some other ensemble methods.   \n",
    "Disadvantages:\n",
    "\n",
    "Can be sensitive to noise in the data.   \n",
    "Computationally expensive.   \n",
    "Less interpretable than simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df12ed0-72ef-4e0d-bc63-28e5649e3205",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a228945b-43c5-44b7-a45f-a8a49a532910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.611767794200296\n",
      "R-squared: 0.18859857319759554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Simple dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.2, 1.9, 3.1, 3.8, 5.2])\n",
    "\n",
    "class SimpleDecisionStump:\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.feature_index = None\n",
    "        self.left_value = None\n",
    "        self.right_value = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        min_error = float('inf')\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            for threshold in np.unique(X[:, feature_index]):\n",
    "                left_mask = X[:, feature_index] < threshold\n",
    "                right_mask = X[:, feature_index] >= threshold\n",
    "\n",
    "                # Calculate means only for non-empty slices\n",
    "                if np.sum(left_mask) > 0:\n",
    "                    left_value = np.mean(y[left_mask])\n",
    "                else:\n",
    "                    left_value = 0  # Default value if no data in this group\n",
    "\n",
    "                if np.sum(right_mask) > 0:\n",
    "                    right_value = np.mean(y[right_mask])\n",
    "                else:\n",
    "                    right_value = 0  # Default value if no data in this group\n",
    "\n",
    "                predictions = np.where(left_mask, left_value, right_value)\n",
    "                error = mean_squared_error(y, predictions)\n",
    "\n",
    "                if error < min_error:\n",
    "                    self.threshold = threshold\n",
    "                    self.feature_index = feature_index\n",
    "                    self.left_value = left_value\n",
    "                    self.right_value = right_value\n",
    "                    min_error = error\n",
    "\n",
    "    def predict(self, X):\n",
    "        left_mask = X[:, self.feature_index] < self.threshold\n",
    "        predictions = np.where(left_mask, self.left_value, self.right_value)\n",
    "        return predictions\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.zeros_like(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            stump = SimpleDecisionStump()\n",
    "            stump.fit(X, residuals)\n",
    "            self.models.append(stump)\n",
    "            y_pred += self.learning_rate * stump.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Instantiate and train the model\n",
    "gb = GradientBoosting(n_estimators=10, learning_rate=0.1)\n",
    "gb.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb.predict(X)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ea6a1-8c5a-41c6-b222-e75ecb7db4bd",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth toptimise the performance of the model. Use grid search or random search to find the besthyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b32c121-cfb0-4f25-b1ba-c87d03b5f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_estimators=10, learning_rate=0.01, max_depth=1 -> MSE=9.275053535949507, R2=-3.669277857405109\n",
      "Params: n_estimators=10, learning_rate=0.01, max_depth=2 -> MSE=9.192381655399444, R2=-3.6276589082759996\n",
      "Params: n_estimators=10, learning_rate=0.01, max_depth=3 -> MSE=9.183459095341707, R2=-3.623167083840973\n",
      "Params: n_estimators=10, learning_rate=0.1, max_depth=1 -> MSE=1.611767794200296, R2=0.18859857319759554\n",
      "Params: n_estimators=10, learning_rate=0.1, max_depth=2 -> MSE=1.3852993028613583, R2=0.3026080835373749\n",
      "Params: n_estimators=10, learning_rate=0.1, max_depth=3 -> MSE=1.365062677742912, R2=0.31279567169607725\n",
      "Params: n_estimators=10, learning_rate=0.2, max_depth=1 -> MSE=0.22698111602427967, R2=0.8857324224605921\n",
      "Params: n_estimators=10, learning_rate=0.2, max_depth=2 -> MSE=0.13439205213083597, R2=0.932343912539853\n",
      "Params: n_estimators=10, learning_rate=0.2, max_depth=3 -> MSE=0.12945002653725673, R2=0.9348318432655776\n",
      "Params: n_estimators=20, learning_rate=0.01, max_depth=1 -> MSE=7.671740000402345, R2=-2.8621325012093966\n",
      "Params: n_estimators=20, learning_rate=0.01, max_depth=2 -> MSE=7.526394199369446, R2=-2.7889620415673817\n",
      "Params: n_estimators=20, learning_rate=0.01, max_depth=3 -> MSE=7.5112149052203705, R2=-2.7813204315446898\n",
      "Params: n_estimators=20, learning_rate=0.1, max_depth=1 -> MSE=0.2754760721142639, R2=0.8613189326851269\n",
      "Params: n_estimators=20, learning_rate=0.1, max_depth=2 -> MSE=0.17185372777759497, R2=0.9134848329754355\n",
      "Params: n_estimators=20, learning_rate=0.1, max_depth=3 -> MSE=0.1659597536664276, R2=0.9164519967446498\n",
      "Params: n_estimators=20, learning_rate=0.2, max_depth=1 -> MSE=0.00953141741202567, R2=0.9952016625996649\n",
      "Params: n_estimators=20, learning_rate=0.2, max_depth=2 -> MSE=0.0016291542745958364, R2=0.9991798458142389\n",
      "Params: n_estimators=20, learning_rate=0.2, max_depth=3 -> MSE=0.0014924571936673126, R2=0.9992486623068529\n",
      "Params: n_estimators=50, learning_rate=0.01, max_depth=1 -> MSE=4.375843152289856, R2=-1.2029013050190578\n",
      "Params: n_estimators=50, learning_rate=0.01, max_depth=2 -> MSE=4.134203403383545, R2=-1.0812542304588932\n",
      "Params: n_estimators=50, learning_rate=0.01, max_depth=3 -> MSE=4.109811127815821, R2=-1.0689745911275783\n",
      "Params: n_estimators=50, learning_rate=0.1, max_depth=1 -> MSE=0.0034502641899317016, R2=0.9982630566905297\n",
      "Params: n_estimators=50, learning_rate=0.1, max_depth=2 -> MSE=0.0003323334233798654, R2=0.999832695618516\n",
      "Params: n_estimators=50, learning_rate=0.1, max_depth=3 -> MSE=0.00029823138670983195, R2=0.9998498633776128\n",
      "Params: n_estimators=50, learning_rate=0.2, max_depth=1 -> MSE=2.7964646607347676e-06, R2=0.9999985921945929\n",
      "Params: n_estimators=50, learning_rate=0.2, max_depth=2 -> MSE=3.0785811870321195e-09, R2=0.9999999984501705\n",
      "Params: n_estimators=50, learning_rate=0.2, max_depth=3 -> MSE=2.2871839942345867e-09, R2=0.9999999988485784\n",
      "\n",
      "Best Model Parameters:\n",
      "n_estimators: 50, learning_rate: 0.2, max_depth: 3\n",
      "Best MSE: 2.2871839942345867e-09\n",
      "R-squared: 0.9999999988485784\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Simple dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.2, 1.9, 3.1, 3.8, 5.2])\n",
    "\n",
    "class SimpleDecisionStump:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.threshold = None\n",
    "        self.feature_index = None\n",
    "        self.left_value = None\n",
    "        self.right_value = None\n",
    "        self.is_leaf = False\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        if len(np.unique(y)) == 1 or depth == self.max_depth:\n",
    "            self.is_leaf = True\n",
    "            self.left_value = np.mean(y)\n",
    "            return\n",
    "\n",
    "        min_error = float('inf')\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            for threshold in np.unique(X[:, feature_index]):\n",
    "                left_mask = X[:, feature_index] < threshold\n",
    "                right_mask = X[:, feature_index] >= threshold\n",
    "\n",
    "                if np.sum(left_mask) > 0:\n",
    "                    left_value = np.mean(y[left_mask])\n",
    "                else:\n",
    "                    left_value = 0\n",
    "\n",
    "                if np.sum(right_mask) > 0:\n",
    "                    right_value = np.mean(y[right_mask])\n",
    "                else:\n",
    "                    right_value = 0\n",
    "\n",
    "                predictions = np.where(left_mask, left_value, right_value)\n",
    "                error = mean_squared_error(y, predictions)\n",
    "\n",
    "                if error < min_error:\n",
    "                    self.threshold = threshold\n",
    "                    self.feature_index = feature_index\n",
    "                    self.left_value = left_value\n",
    "                    self.right_value = right_value\n",
    "                    min_error = error\n",
    "\n",
    "        if depth < self.max_depth:\n",
    "            self.left_child = SimpleDecisionStump(max_depth=self.max_depth)\n",
    "            self.right_child = SimpleDecisionStump(max_depth=self.max_depth)\n",
    "            left_mask = X[:, self.feature_index] < self.threshold\n",
    "            self.left_child.fit(X[left_mask], y[left_mask], depth + 1)\n",
    "            self.right_child.fit(X[~left_mask], y[~left_mask], depth + 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.is_leaf:\n",
    "            return np.full(X.shape[0], self.left_value)\n",
    "        left_mask = X[:, self.feature_index] < self.threshold\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        if self.left_child:\n",
    "            predictions[left_mask] = self.left_child.predict(X[left_mask])\n",
    "        if self.right_child:\n",
    "            predictions[~left_mask] = self.right_child.predict(X[~left_mask])\n",
    "        return predictions\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.zeros_like(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            stump = SimpleDecisionStump(max_depth=self.max_depth)\n",
    "            stump.fit(X, residuals)\n",
    "            self.models.append(stump)\n",
    "            y_pred += self.learning_rate * stump.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "def grid_search(X, y, param_grid):\n",
    "    best_mse = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            for max_depth in param_grid['max_depth']:\n",
    "                model = GradientBoosting(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "                model.fit(X, y)\n",
    "                y_pred = model.predict(X)\n",
    "                mse = mean_squared_error(y, y_pred)\n",
    "                r2 = r2_score(y, y_pred)\n",
    "                \n",
    "                print(f\"Params: n_estimators={n_estimators}, learning_rate={learning_rate}, max_depth={max_depth} -> MSE={mse}, R2={r2}\")\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_params = (n_estimators, learning_rate, max_depth)\n",
    "                    best_model = model\n",
    "\n",
    "    return best_model, best_params, best_mse\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_model, best_params, best_mse = grid_search(X, y, param_grid)\n",
    "\n",
    "print(\"\\nBest Model Parameters:\")\n",
    "print(f\"n_estimators: {best_params[0]}, learning_rate: {best_params[1]}, max_depth: {best_params[2]}\")\n",
    "print(f\"Best MSE: {best_mse}\")\n",
    "\n",
    "# Predictions with the best model\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Evaluate performance\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64f2b8-0d2b-4088-9a0e-9feb79154981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d06587d-43a2-4545-b9a9-022f2a4b1927",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A weak learner in Gradient Boosting is a simple model that performs slightly better than random guessing. It's typically a decision tree with a limited depth (often just one or two levels), also known as a decision stump. While individually weak, these models collectively form a powerful ensemble when combined through the gradient boosting process.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to iteratively improve a model by focusing on its errors. It works on the principle of gradient descent, where the goal is to minimize a loss function. Each new model is trained to predict the residuals (errors) of the previous model. By sequentially adding these weak models, the overall prediction accuracy improves.   \n",
    "\n",
    "Key idea: Correct the mistakes made by previous models.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Initialize: Start with a simple model (e.g., a constant value).\n",
    "Calculate residuals: Determine the errors between the predicted and actual values.\n",
    "Fit a weak learner: Train a weak learner (decision tree) to predict the residuals.\n",
    "Update model: Add the predictions of the new weak learner to the existing model.\n",
    "Repeat: Iterate steps 2-4 for a specified number of times or until a stopping criterion is met.\n",
    "The final model is a combination of all the weak learners, where each contributes to correcting the errors of the previous ones.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "While a full mathematical derivation is complex, the core idea involves:\n",
    "\n",
    "Defining a loss function: This measures the error between the predicted and actual values. Common loss functions include mean squared error (MSE) for regression and log-loss for classification.\n",
    "Calculating gradients: Determine the gradient of the loss function with respect to the model's predictions. This indicates the direction to improve the model.\n",
    "Fitting weak learners: Train weak learners to minimize the negative gradient (steepest descent).\n",
    "Updating model: Add the predictions of the new weak learner multiplied by a learning rate to the existing model.\n",
    "Iterative process: Repeat steps 2-4 until convergence or a specified number of iterations.\n",
    "The mathematical details involve calculus and optimization techniques, but the core concept is to iteratively minimize the loss function using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15b8e1-3919-440f-81ff-db0033210863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
